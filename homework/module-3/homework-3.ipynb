{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge4C2c2_w7Ac"
   },
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ge4C2c2_w7Ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AsPVf4XT1JAZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>growth_brent_oil_7d</th>\n",
       "      <th>growth_brent_oil_30d</th>\n",
       "      <th>growth_brent_oil_90d</th>\n",
       "      <th>growth_brent_oil_365d</th>\n",
       "      <th>growth_btc_usd_1d</th>\n",
       "      <th>growth_btc_usd_3d</th>\n",
       "      <th>growth_btc_usd_7d</th>\n",
       "      <th>growth_btc_usd_30d</th>\n",
       "      <th>growth_btc_usd_90d</th>\n",
       "      <th>growth_btc_usd_365d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.101563</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>1.031789e+09</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.062311</td>\n",
       "      <td>3.081600e+08</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.063386</td>\n",
       "      <td>1.331712e+08</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.098958</td>\n",
       "      <td>0.099826</td>\n",
       "      <td>0.061774</td>\n",
       "      <td>6.776640e+07</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099826</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.098090</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>4.789440e+07</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>1986</td>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close  Adj Close_x        Volume Ticker  \\\n",
       "0  0.088542  0.101563  0.088542  0.097222     0.060163  1.031789e+09   MSFT   \n",
       "1  0.097222  0.102431  0.097222  0.100694     0.062311  3.081600e+08   MSFT   \n",
       "2  0.100694  0.103299  0.100694  0.102431     0.063386  1.331712e+08   MSFT   \n",
       "3  0.102431  0.103299  0.098958  0.099826     0.061774  6.776640e+07   MSFT   \n",
       "4  0.099826  0.100694  0.097222  0.098090     0.060700  4.789440e+07   MSFT   \n",
       "\n",
       "   Year      Month  Weekday  ... growth_brent_oil_7d  growth_brent_oil_30d  \\\n",
       "0  1986 1986-03-01        3  ...                 NaN                   NaN   \n",
       "1  1986 1986-03-01        4  ...                 NaN                   NaN   \n",
       "2  1986 1986-03-01        0  ...                 NaN                   NaN   \n",
       "3  1986 1986-03-01        1  ...                 NaN                   NaN   \n",
       "4  1986 1986-03-01        2  ...                 NaN                   NaN   \n",
       "\n",
       "   growth_brent_oil_90d  growth_brent_oil_365d  growth_btc_usd_1d  \\\n",
       "0                   NaN                    NaN                NaN   \n",
       "1                   NaN                    NaN                NaN   \n",
       "2                   NaN                    NaN                NaN   \n",
       "3                   NaN                    NaN                NaN   \n",
       "4                   NaN                    NaN                NaN   \n",
       "\n",
       "   growth_btc_usd_3d  growth_btc_usd_7d  growth_btc_usd_30d  \\\n",
       "0                NaN                NaN                 NaN   \n",
       "1                NaN                NaN                 NaN   \n",
       "2                NaN                NaN                 NaN   \n",
       "3                NaN                NaN                 NaN   \n",
       "4                NaN                NaN                 NaN   \n",
       "\n",
       "   growth_btc_usd_90d  growth_btc_usd_365d  \n",
       "0                 NaN                  NaN  \n",
       "1                 NaN                  NaN  \n",
       "2                 NaN                  NaN  \n",
       "3                 NaN                  NaN  \n",
       "4                 NaN                  NaN  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_parquet(\"D:\\zoomcamp\\myproject\\stock-market-analytics\\module3\\downloadsfchkomj5.part\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tZF13UbOXGF",
    "outputId": "4a18459a-8d10-4d64-f5f0-9d6ee2ddfa5d"
   },
   "outputs": [],
   "source": [
    "GROWTH = [g for g in df_full.keys() if (g.find('growth_')==0)&(g.find('future')<0)]\n",
    "OHLCV = ['Open','High','Low','Close','Adj Close_x','Volume']\n",
    "CATEGORICAL = ['Month', 'Weekday', 'Ticker', 'ticker_type']\n",
    "TO_PREDICT = [g for g in df_full.keys() if (g.find('future')>=0)]\n",
    "TO_DROP = ['Year','Date','index_x', 'index_y', 'index', 'Quarter','Adj Close_y'] + CATEGORICAL + OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VgBFjAADfXSp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garjita\\AppData\\Local\\Temp\\ipykernel_26064\\912909511.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  df_full['ln_volume'] = df_full.Volume.apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "df_full['ln_volume'] = df_full.Volume.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lukhBLIcfW8g"
   },
   "outputs": [],
   "source": [
    "# manually defined features\n",
    "CUSTOM_NUMERICAL = ['SMA10', 'SMA20', 'growing_moving_average', 'high_minus_low_relative','volatility', 'ln_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IpRKmVtaf6is"
   },
   "outputs": [],
   "source": [
    "# All Supported Ta-lib indicators: https://github.com/TA-Lib/ta-lib-python/blob/master/docs/funcs.md\n",
    "\n",
    "TECHNICAL_INDICATORS = ['adx', 'adxr', 'apo', 'aroon_1','aroon_2', 'aroonosc',\n",
    " 'bop', 'cci', 'cmo','dx', 'macd', 'macdsignal', 'macdhist', 'macd_ext',\n",
    " 'macdsignal_ext', 'macdhist_ext', 'macd_fix', 'macdsignal_fix',\n",
    " 'macdhist_fix', 'mfi', 'minus_di', 'mom', 'plus_di', 'dm', 'ppo',\n",
    " 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'slowk', 'slowd', 'fastk',\n",
    " 'fastd', 'fastk_rsi', 'fastd_rsi', 'trix', 'ultosc', 'willr',\n",
    " 'ad', 'adosc', 'obv', 'atr', 'natr', 'ht_dcperiod', 'ht_dcphase',\n",
    " 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine',\n",
    " 'ht_trendmod', 'avgprice', 'medprice', 'typprice', 'wclprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncPdBnx13ilm",
    "outputId": "2f988be7-3810-4c1b-e342-ec48a3febb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical patterns count = 61, examples = ['cdl2crows', 'cdl3blackrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside']\n"
     ]
    }
   ],
   "source": [
    "TECHNICAL_PATTERNS = [g for g in df_full.keys() if g.find('cdl')>=0]\n",
    "print(f'Technical patterns count = {len(TECHNICAL_PATTERNS)}, examples = {TECHNICAL_PATTERNS[0:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ri9F9mIwIuVa",
    "outputId": "1a3aee8b-61e4-455f-b771-28860bb8133b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['growth_future_5d', 'is_positive_growth_5d_future']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MACRO = ['gdppot_us_yoy', 'gdppot_us_qoq', 'cpi_core_yoy', 'cpi_core_mom', 'FEDFUNDS', 'DGS1', 'DGS5', 'DGS10']\n",
    "NUMERICAL = GROWTH + TECHNICAL_INDICATORS + TECHNICAL_PATTERNS + CUSTOM_NUMERICAL + MACRO\n",
    "\n",
    "# CHECK: NO OTHER INDICATORS LEFT\n",
    "OTHER = [k for k in df_full.keys() if k not in OHLCV + CATEGORICAL + NUMERICAL + TO_DROP]\n",
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zv_OOrK1ulV",
    "outputId": "f35730f6-379e-45af-8e3c-4afaff6e1546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.Ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmIpnwsJLvtI",
    "outputId": "eff90f87-3b42-4e22-ec0a-8ec75d79ebdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 182675 entries, 3490 to 5426\n",
      "Columns: 203 entries, Open to ln_volume\n",
      "dtypes: datetime64[ns](3), float64(129), int32(64), int64(5), object(2)\n",
      "memory usage: 239.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# truncated df_full with 25 years of data (and defined growth variables)\n",
    "df = df_full[df_full.Date>='2000-01-01']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pa-AoEDeLX4x"
   },
   "outputs": [],
   "source": [
    "# dummy variables are not generated from Date and numeric variables\n",
    "df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
    "df.loc[:,'Weekday'] = df.Weekday.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tuAlw48XKjE5"
   },
   "outputs": [],
   "source": [
    "# Generate dummy variables (no need for bool, let's have int32 instead)\n",
    "dummy_variables = pd.get_dummies(df[CATEGORICAL], dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies names in a list\n",
    "DUMMIES = dummy_variables.keys().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (1 point): Dummies on Month and Week-of-Month\n",
    "Find the ABSOLUTE CORRELATION VALUE of the most correlated dummy with the binary outcome variable is_positive_growth_5d_future?\n",
    "\n",
    "You saw in the correlation analysis and modeling that September and October may be important seasonal months. In this task, we'll go futher and try to generate dummies for Month and Week-of-month (starting from 1). For example, the first week of October should be coded similar to this: 'October_w1'. Once you've generated the new set of variables, find the most correlated (in absolute value) one with is_positive_growth_5d_future and round it to 3 digits after the comma.\n",
    "\n",
    "Suggested path to a solution:\n",
    "\n",
    "[Source] Use this formula to get the week of month for the datetime variable d: (d.day-1)//7+1 Define a new string variable for all month-week_of_month combinations. Append it to the CATEGORICAL features set. You should have 5 variables treated as CATEGORICAL now: 'Month', 'Weekday', 'Ticker', 'ticker_type', 'month_wom'. In the end, you should get 115 dummy features, including 60 (=12*5) week_month_of_week dummies. Use pandas.get_dummies() to generate dummies. Use pandas.DataFrame.corr() function (also used in [Code Snippet 1]) to get correlations with is_positive_growth_5d_future, filter out only variables representing the new dummy set, and sort it by absolute values (you can define a new column \"abs_corr\" in the dataframe with correlations), and find the highest value (among the new dummies features set). NOTE: new dummies will be used as features in the next tasks, please leave them in the dataset.\n",
    "\n",
    "**Question 1. [Dummies on Month and Week-of-Month] Top correlation of a new dummy with a boolean prediction variable? (1 point)**\n",
    "\n",
    "- 0.025\n",
    "\n",
    "- 0.035\n",
    "\n",
    "- 0.045\n",
    "\n",
    "- 0.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dovxcVkk-72S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490    January_w1\n",
       "3491    January_w1\n",
       "3492    January_w1\n",
       "3493    January_w1\n",
       "3494    January_w1\n",
       "           ...    \n",
       "5422      April_w5\n",
       "5423        May_w1\n",
       "5424        May_w1\n",
       "5425        May_w1\n",
       "5426        May_w1\n",
       "Name: Date, Length: 182675, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 1: define more categorical features, e.g. all combinations for <September+weekday>  (you'll see that September is actually an important dummy in one of the models)\n",
    "week_of_month  = ((df[\"Date\"].dt.day - 1) // 7) + 1\n",
    "month_week_of_month = df[\"Date\"].dt.month_name().str.cat(week_of_month.astype(str), sep=\"_w\")\n",
    "month_week_of_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_week_of_month_dummy = pd.get_dummies(month_week_of_month)\n",
    "correlations = month_week_of_month_dummy.corrwith(df[\"is_positive_growth_5d_future\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_correlation = round(abs(correlations[[correlations.abs().idxmax()]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlation of a new dummy with a boolean prediction variable:\n",
      " September_w3    0.035\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top correlation of a new dummy with a boolean prediction variable:\\n\", top_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer of Question 1 : 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (2 points): Define new \"hand\" rules on macro and technical indicators variables\n",
    "What is the precision score for the best of the NEW predictions (pred3 or pred4), rounded to 3 digits after the comma?\n",
    "\n",
    "Let's utilize the knowledge from the visualised tree (clf10) (Code Snippet 5: 1.4.4 Visualisation):\n",
    "\n",
    "You're asked to define two new 'hand' rules (leading to 'positive' subtrees):\n",
    "\n",
    "pred3_manual_gdp_fastd: (gdppot_us_yoy <= 0.027) & (fastd >= 0.251) pred4_manual_gdp_wti_oil: (gdppot_us_yoy >= 0.027) & (growth_wti_oil_30d <= 1.005) Extend the Code Snippet 3 (Manual \"hand rule\" predictions): Calculate and add new rules (pred3 and pred4) to the dataframe.You should notice that one of the predictions doesn't have any positive predictions on TEST dataset (while it has many on TRAIN+VALIDATION).\n",
    "\n",
    "Debug: check in the new_df and the original dataset/data generation process that we didn't make any mistakes during the data transformation step.\n",
    "\n",
    "Explain why this can happen even if there are no errors in the data features.\n",
    "\n",
    "As a result, write down the precision score for the remaining predictor (round to three decimal points). E.g. if you have 0.57897, your answer should be 0.579.\n",
    "\n",
    "**Question 2. [Define new 'hand' rules] What's the precision score for the best new rule? (2 points)**\n",
    "\n",
    "- 0.575\n",
    "\n",
    "- 0.591\n",
    "\n",
    "- 0.612\n",
    "\n",
    "- 0.555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j0e65F71Kl15"
   },
   "outputs": [],
   "source": [
    "# Concatenate the dummy variables with the original DataFrame\n",
    "df_with_dummies = pd.concat([df, dummy_variables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Qim1MIwUWE9",
    "outputId": "ced7556a-1de9-4cbf-abbd-4e79b8235501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 182675 entries, 3490 to 5426\n",
      "Columns: 239 entries, growth_1d to ticker_type_US\n",
      "dtypes: float64(121), int32(117), int64(1)\n",
      "memory usage: 253.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_with_dummies[NUMERICAL+DUMMIES].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "loo6ktrtvKrn"
   },
   "outputs": [],
   "source": [
    "def temporal_split(df, min_date, max_date, train_prop=0.7, val_prop=0.15, test_prop=0.15):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into three buckets based on the temporal order of the 'Date' column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to split.\n",
    "        min_date (str or Timestamp): Minimum date in the DataFrame.\n",
    "        max_date (str or Timestamp): Maximum date in the DataFrame.\n",
    "        train_prop (float): Proportion of data for training set (default: 0.6).\n",
    "        val_prop (float): Proportion of data for validation set (default: 0.2).\n",
    "        test_prop (float): Proportion of data for test set (default: 0.2).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame with a new column 'split' indicating the split for each row.\n",
    "    \"\"\"\n",
    "    # Define the date intervals\n",
    "    train_end = min_date + pd.Timedelta(days=(max_date - min_date).days * train_prop)\n",
    "    val_end = train_end + pd.Timedelta(days=(max_date - min_date).days * val_prop)\n",
    "\n",
    "    # Assign split labels based on date ranges\n",
    "    split_labels = []\n",
    "    for date in df['Date']:\n",
    "        if date <= train_end:\n",
    "            split_labels.append('train')\n",
    "        elif date <= val_end:\n",
    "            split_labels.append('validation')\n",
    "        else:\n",
    "            split_labels.append('test')\n",
    "\n",
    "    # Add 'split' column to the DataFrame\n",
    "    df['split'] = split_labels\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G0gVQMUEyNIc"
   },
   "outputs": [],
   "source": [
    "min_date_df = df_with_dummies.Date.min()\n",
    "max_date_df = df_with_dummies.Date.max()\n",
    "\n",
    "df_with_dummies = temporal_split(df_with_dummies,\n",
    "                                 min_date = min_date_df,\n",
    "                                 max_date = max_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c2qCynDQBMpV"
   },
   "outputs": [],
   "source": [
    "# remove the \"segmentation\" problem (warning message on df performance after many joins and data transformations)\n",
    "new_df = df_with_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train (152846, 242),  X_test (29829, 242)\n",
      "length: X_train_imputed (152846, 242),  X_test_imputed (29829, 242)\n",
      "Precision score for the best predictor: 0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets based on the split date\n",
    "features_list = NUMERICAL+DUMMIES\n",
    "to_predict = 'is_positive_growth_5d_future'\n",
    "\n",
    "train_df = new_df[new_df.split.isin(['train','validation'])].copy(deep=True)\n",
    "test_df = new_df[new_df.split.isin(['test'])].copy(deep=True)\n",
    "\n",
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "# need Date and Ticker later when merging predictions to the dataset\n",
    "X_train = train_df[features_list+[to_predict,'Date','Ticker']]\n",
    "X_test = test_df[features_list+[to_predict,'Date','Ticker']]\n",
    "\n",
    "print(f'length: X_train {X_train.shape},  X_test {X_test.shape}')\n",
    "\n",
    "#\n",
    "# Can't have +-inf values . E.g. ln(volume)=-inf when volume==0 => substitute with 0\n",
    "\n",
    "# Disable SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Need to fill NaNs somehow\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "print(f'length: X_train_imputed {X_train.shape},  X_test_imputed {X_test.shape}')\n",
    "\n",
    "#\n",
    "# you may want to remove 1-2% outliers based on percentile ==> not used here in Decision Trees\n",
    "def remove_outliers_percentile(X, lower_percentile=1, upper_percentile=99):\n",
    "    \"\"\"\n",
    "    Remove outliers from the input array based on percentiles.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array (NumPy array or array-like)\n",
    "    - lower_percentile: Lower percentile threshold (float, default=1)\n",
    "    - upper_percentile: Upper percentile threshold (float, default=99)\n",
    "\n",
    "    Returns:\n",
    "    - Array with outliers removed\n",
    "    \"\"\"\n",
    "    lower_bound = np.percentile(X, lower_percentile, axis=0)\n",
    "    upper_bound = np.percentile(X, upper_percentile, axis=0)\n",
    "    mask = np.logical_and(np.all(X >= lower_bound, axis=1), np.all(X <= upper_bound, axis=1))\n",
    "    return X[mask]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your input data\n",
    "# filtered_X = remove_outliers_percentile(X, lower_percentile=1, upper_percentile=99)\n",
    "\n",
    "#\n",
    "X_train_imputed = X_train # we won't use outliers removal to save more data to train: remove_outliers_percentile(X_train)\n",
    "X_test_imputed = X_test # we won't use outliers removal to save more data to test: remove_outliers_percentile(X_test)\n",
    "\n",
    "#\n",
    "y_train = X_train_imputed[to_predict]\n",
    "y_test = X_test_imputed[to_predict]\n",
    "\n",
    "# remove y_train, y_test from X_ dataframes\n",
    "del X_train_imputed[to_predict]\n",
    "del X_test_imputed[to_predict]\n",
    "\n",
    "#\n",
    "# Define new 'hand' rules\n",
    "pred3_manual_gdp_fastd = (test_df['gdppot_us_yoy'] <= 0.027) & (test_df['fastd'] >= 0.251)\n",
    "pred4_manual_gdp_wti_oil = (test_df['gdppot_us_yoy'] >= 0.027) & (test_df['growth_wti_oil_30d'] <= 1.005)\n",
    "\n",
    "# Add new rules to the dataframe\n",
    "test_df['pred3'] = pred3_manual_gdp_fastd.astype(int)\n",
    "test_df['pred4'] = pred4_manual_gdp_wti_oil.astype(int)\n",
    "\n",
    "# Evaluate the performance of pred3 and pred4 on the test dataset\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Ensure alignment between test dataset and predictions\n",
    "y_test = y_test[:len(test_df)]\n",
    "\n",
    "# Evaluate the performance of pred3 and pred4 on the test dataset\n",
    "precision_pred3 = precision_score(y_test, test_df['pred3'])\n",
    "precision_pred4 = precision_score(y_test, test_df['pred4'])\n",
    "\n",
    "# Choose the predictor with the highest precision score\n",
    "best_precision = max(precision_pred3, precision_pred4)\n",
    "\n",
    "# Round the precision score to 3 decimal points\n",
    "rounded_precision = round(best_precision, 3)\n",
    "\n",
    "print(\"Precision score for the best predictor:\", rounded_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer of Question 2 : 0.555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (1 point): Unique correct predictions from a 10-levels deep Decision Tree Classifier (pred5_clf_10)\n",
    "What is the total number of records in the TEST dataset when the new prediction pred5_clf_10 is better than all 'hand' rules (pred0..pred4)?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure everyone gets the same results.\n",
    "\n",
    "Suggested solution:\n",
    "\n",
    "Step1: Rewrite the '1.4.3 Inference for a decision tree' piece for the Decision Tree Classifier with max_depth=10 (clf_10), so that you fit the model on TRAIN+VALIDATION sets (unchanged from the lecture), but predict on the whole set X_all (to be able to define a new column 'pred5_clf_10' in the dataframe new_df). Here is the link with explanation. It will solve the problem in 1.4.5 when predictions were made only for Test dataset and couldn't be easily joined with the full dataset.\n",
    "\n",
    "Step2: Once you have it, define a new column 'only_pred5_is_correct' similar to 'hand' prediction rules with several conditions: is_positive_growth_5d_future AND is_correct_pred5 should be equal 1, while all other predictions is_correct_pred0..is_correct_pred4 should be equal to 0.\n",
    "\n",
    "Step3: Convert 'only_pred5_is_correct' column from bool to int, and find how many times it is equal to 1 in the TEST set. Write down this as an answer.\n",
    "\n",
    "ADVANCED: define a function that can be applied to the whole row of predictions (a few examples of pandas-apply-row-functions) and can find whether some prediction 'predX' (where X is one of the predictions) is uniquely correct. It should work even if there are 100 predictions available, so that you don't define manually the condition for 'predX'.\n",
    "\n",
    "**Question 3. [Unique correct predictions for pred5_clf_10] Wha'ts the total number of records of 'unique correctness' on TEST? (1 point)**\n",
    "\n",
    "- 1\n",
    "\n",
    "- 5\n",
    "\n",
    "- 10\n",
    "\n",
    "- 15\n",
    "\n",
    "- 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lMHrM1sdLdYc"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oYKW8lodlM7"
   },
   "source": [
    "### 1.4.1) Define dataframes AND perform data cleaning\n",
    "* define X_train (dataframe), X_test (dataframe), y_train (series), y_test (series)\n",
    "* replace +-inf. with 0\n",
    "* fill NaNs with 0 (you can drop it too, but will loose a lot of data in our case\n",
    "* remove 1-2% outliers (in each dimension, or only in variable to_predict :: we won't use it for a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nNjsv3RtNjCu"
   },
   "outputs": [],
   "source": [
    "# Decision Tree doesn't like too large and inf. values\n",
    "import numpy as np\n",
    "\n",
    "def remove_infinite_values(X):\n",
    "    \"\"\"\n",
    "    Remove infinite values from the input array.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array (NumPy array or array-like)\n",
    "\n",
    "    Returns:\n",
    "    - Array with infinite values removed\n",
    "    \"\"\"\n",
    "    return X[np.isfinite(X).all(axis=1)]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your input data\n",
    "# filtered_X = remove_infinite_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoiZYkfhQa_g",
    "outputId": "e45ab692-4426-4162-d32c-3c71b743051c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train (152846, 242),  X_test (29829, 242)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets based on the split date\n",
    "features_list = NUMERICAL+DUMMIES\n",
    "to_predict = 'is_positive_growth_5d_future'\n",
    "\n",
    "train_df = new_df[new_df.split.isin(['train','validation'])].copy(deep=True)\n",
    "test_df = new_df[new_df.split.isin(['test'])].copy(deep=True)\n",
    "\n",
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "# need Date and Ticker later when merging predictions to the dataset\n",
    "X_train = train_df[features_list+[to_predict,'Date','Ticker']]\n",
    "X_test = test_df[features_list+[to_predict,'Date','Ticker']]\n",
    "\n",
    "print(f'length: X_train {X_train.shape},  X_test {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46GYMy7KRUB3",
    "outputId": "74d00099-fba8-4417-e778-6e16e0241c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train_imputed (152846, 242),  X_test_imputed (29829, 242)\n"
     ]
    }
   ],
   "source": [
    "# Can't have +-inf values . E.g. ln(volume)=-inf when volume==0 => substitute with 0\n",
    "\n",
    "# Disable SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Need to fill NaNs somehow\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "print(f'length: X_train_imputed {X_train.shape},  X_test_imputed {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GiJKy6vPWB19"
   },
   "outputs": [],
   "source": [
    "# you may want to remove 1-2% outliers based on percentile ==> not used here in Decision Trees\n",
    "def remove_outliers_percentile(X, lower_percentile=1, upper_percentile=99):\n",
    "    \"\"\"\n",
    "    Remove outliers from the input array based on percentiles.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array (NumPy array or array-like)\n",
    "    - lower_percentile: Lower percentile threshold (float, default=1)\n",
    "    - upper_percentile: Upper percentile threshold (float, default=99)\n",
    "\n",
    "    Returns:\n",
    "    - Array with outliers removed\n",
    "    \"\"\"\n",
    "    lower_bound = np.percentile(X, lower_percentile, axis=0)\n",
    "    upper_bound = np.percentile(X, upper_percentile, axis=0)\n",
    "    mask = np.logical_and(np.all(X >= lower_bound, axis=1), np.all(X <= upper_bound, axis=1))\n",
    "    return X[mask]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your input data\n",
    "# filtered_X = remove_outliers_percentile(X, lower_percentile=1, upper_percentile=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8-NNrhAKWXr8"
   },
   "outputs": [],
   "source": [
    "X_train_imputed = X_train # we won't use outliers removal to save more data to train: remove_outliers_percentile(X_train)\n",
    "X_test_imputed = X_test # we won't use outliers removal to save more data to test: remove_outliers_percentile(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBXvMVKWWgSc",
    "outputId": "5424a261-b9af-4f9d-d5d9-c38753440ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train_imputed (152846, 242),  X_test_imputed (29829, 242)\n"
     ]
    }
   ],
   "source": [
    "# same shape\n",
    "print(f'length: X_train_imputed {X_train_imputed.shape},  X_test_imputed {X_test_imputed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "E6A_zhDLW0aC"
   },
   "outputs": [],
   "source": [
    "y_train = X_train_imputed[to_predict]\n",
    "y_test = X_test_imputed[to_predict]\n",
    "\n",
    "# remove y_train, y_test from X_ dataframes\n",
    "del X_train_imputed[to_predict]\n",
    "del X_test_imputed[to_predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUGSkY7yeO5F"
   },
   "source": [
    "### 1.4.2 Estimation of a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tsSxJEireSeW"
   },
   "outputs": [],
   "source": [
    "# INPUTS:\n",
    "# X_train_imputed : CLEAN dataFrame with only numerical features (train+validation periods)\n",
    "# X_test_imputed : CLEAN dataFrame with only numerical features (test periods)\n",
    "\n",
    "# y_train : true values for the train period\n",
    "# y_test  : true values for the test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QcJAw5w9R7pA"
   },
   "outputs": [],
   "source": [
    "# estimation/fit function (using dataframe of features X and what to predict y) --> optimising total accuracy\n",
    "# max_depth is hyperParameter\n",
    "def fit_decision_tree(X, y, max_depth=20):\n",
    "# Initialize the Decision Tree Classifier\n",
    "  clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "\n",
    "  # Fit the classifier to the training data\n",
    "  clf.fit(X, y)\n",
    "  return clf, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClqSfId-gbMW",
    "outputId": "3fb41805-a03f-479d-94ed-b3262402c261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_10, train_columns = fit_decision_tree(X=X_train_imputed.drop(['Date','Ticker'],axis=1),\n",
    "                           y=y_train,\n",
    "                           max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "x74N7Brb-IAm"
   },
   "outputs": [],
   "source": [
    "# TODO 3: TRAIN only on train dataset, experiment with trees with depth 1..20 --> find the best one on VALID dataset\n",
    "#       for the \"best\" tree model: find precision on the TEST set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QbUrS7Ygm2G"
   },
   "source": [
    "### 1.4.3 Inference for a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "csUq1CWISRCe"
   },
   "outputs": [],
   "source": [
    "def predict_decision_tree(clf:DecisionTreeClassifier, df_X:pd.DataFrame, y_true: pd.Series):\n",
    "  # Predict the target variable on the test data\n",
    "  y_pred = clf.predict(df_X)\n",
    "\n",
    "  max_depth = clf.tree_.max_depth\n",
    "  # Print the maximum depth\n",
    "  print(\"Maximum depth of the decision tree:\", max_depth)\n",
    "\n",
    "  # Calculate the accuracy/precision of the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  print(f'Accuracy ={accuracy}, precision = {precision}')\n",
    "\n",
    "  # resulting df\n",
    "  result_df = pd.concat([df_X, y_true, pd.Series(y_pred, index=df_X.index, name='pred_')], axis=1)\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "84F-Mk1P2sGb"
   },
   "outputs": [],
   "source": [
    "# Feautures importance function to predict future returns (based on the classifier)\n",
    "# get feature importance from 'clf' (classifier) and 'train_columns' (column names)\n",
    "\n",
    "def get_importances(clf, train_columns):\n",
    "  # Assuming clf is your trained DecisionTreeClassifier\n",
    "  feature_importance = clf.feature_importances_\n",
    "\n",
    "  # Assuming X_train is your training features\n",
    "  feature_names = train_columns\n",
    "\n",
    "  # Create a DataFrame to store feature importance\n",
    "  feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "  # Sort the DataFrame by importance in descending order\n",
    "  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "  # Print or display the feature importance DataFrame\n",
    "  # print(feature_importance_df)\n",
    "  return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "z3scA_5KnLbZ"
   },
   "outputs": [],
   "source": [
    "# TODO 4: JOIN predictions with the original dataframe (define a new column):\n",
    "#  so, that there are columns pred_tree_clf10 AND pred_tree_clf20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "dtype: int32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = clf_10.predict(X_test_imputed.drop(['Date','Ticker'],axis=1))\n",
    "tp_series = pd.Series(test_pred, dtype=int)\n",
    "tp_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = new_df[\"split\"] == \"test\"\n",
    "new_df.loc[test_mask, \"pred5_clf_10\"] = tp_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0_to_4 = new_df.loc[test_mask, new_df.columns.str.contains(r\"^pred[0-4]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_all_wrong = (pred0_to_4.values != new_df.loc[test_mask, to_predict].values[:, np.newaxis]).all(axis=1)\n",
    "pred_5_correct = new_df.loc[test_mask, to_predict].values == new_df.loc[test_mask, \"pred5_clf_10\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16168"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(manual_all_wrong & pred_5_correct).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred5_clf_10</th>\n",
       "      <th>is_positive_growth_5d_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8704</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8705</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5413</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5414</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5416</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16168 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred5_clf_10  is_positive_growth_5d_future\n",
       "8700           1.0                             1\n",
       "8701           1.0                             1\n",
       "8702           1.0                             1\n",
       "8704           1.0                             1\n",
       "8705           1.0                             1\n",
       "...            ...                           ...\n",
       "5412           1.0                             1\n",
       "5413           1.0                             1\n",
       "5414           1.0                             1\n",
       "5415           1.0                             1\n",
       "5416           1.0                             1\n",
       "\n",
       "[16168 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cols = new_df.columns[new_df.columns.str.contains(r\"^pred\")].to_list()\n",
    "new_df.loc[test_mask, pred_cols + [to_predict]][manual_all_wrong & pred_5_correct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_forpred5_clf_10 = new_df.loc[test_mask, pred_cols + [to_predict]][manual_all_wrong & pred_5_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8700    1\n",
       "8701    1\n",
       "8702    1\n",
       "8704    1\n",
       "8705    1\n",
       "       ..\n",
       "5412    1\n",
       "5413    1\n",
       "5414    1\n",
       "5415    1\n",
       "5416    1\n",
       "Name: is_positive_growth_5d_future, Length: 16168, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_forpred5_clf_10['is_positive_growth_5d_future']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_forpred5_clf_10.is_positive_growth_5d_future.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer of Question 3 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: (2 points) Hyperparameter tuning for a Decision Tree\n",
    "What is the optimal tree depth (from 1 to 20) for a DecisionTreeClassifier?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure consistency in results.\n",
    "\n",
    "Follow these steps to find the optimal max_depth:\n",
    "\n",
    "Iterate through max_depth values from 1 to 20. Train the Decision Tree Classifier with the current max_depth parameter. Optionally, visualize how the 'head' of each fitted tree changes with more advanced (=deep) trees. You can use the sklearn.tree.plot_tree() function, or the compact way with the export_text() functionality (Stack Overflow example): from sklearn.tree import export_text tree_rules = export_text(model, feature_names=list(X_train), max_depth=3) print(tree_rules) Calculate the precision score (you can use the function sklearn.metrics.precision_score()) on the TEST dataset for each of the fitted trees. You can also compare it with the precision score on a VALIDATION dataset, which is included to the training phase (to have more data to train on). You should see that the precision score on a VALIDATION set starts to grow with the complexity of a tree (overfit), which isn't seen on the precision score on TEST. Identify the optimal max_depth, where the precision score is the highest on the TEST dataset. Record this value as best_max_depth and submit as an answer. Make predictions on all records (TRAIN+VALIDATION+TEST) and add the new prediction pred6_clf_best to the dataframe new_df. Additionally, compare the precision score of the tuned decision tree with previous predictions. You should observe an improvement (>0.58, or more than 58% precision), indicating that the tuned tree outperforms previous manual \"hand\" rules and Decision Tree predictions.\n",
    "\n",
    "ADVANCED: Read more about different aspects of scikit-learn Decision Trees. Draw a line of precision/accuracy vs. max_depth and note whether there's a saturation point of precision/accuracy as max_depth increases. In theory, there should be a trade-off between better fitting (=more complex trees) and generalization.\n",
    "\n",
    "**Question 4. [Hyperparams tuning] What's the best max_depth for a Decision Tree Classifier? (2 points)**\n",
    "\n",
    "- 13\n",
    "\n",
    "- 16\n",
    "\n",
    "- 15\n",
    "\n",
    "- 18\n",
    "\n",
    "- 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152846, 239), (152846,), (29829, 239), (29829,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "# need Date and Ticker later when merging predictions to the dataset\n",
    "X_train = new_df.loc[new_df[\"split\"].isin(['train','validation']), features_list]\n",
    "y_train = new_df.loc[new_df[\"split\"].isin(['train','validation']), to_predict]\n",
    "X_test = new_df.loc[new_df[\"split\"] == \"test\", features_list]\n",
    "y_test = new_df.loc[new_df[\"split\"] == \"test\", to_predict]\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Need to fill NaNs somehow\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=1\n",
      "0.555\t0.555\n",
      "max_depth=2\n",
      "0.555\t0.555\n",
      "max_depth=3\n",
      "0.555\t0.555\n",
      "max_depth=4\n",
      "0.555\t0.555\n",
      "max_depth=5\n",
      "0.556\t0.555\n",
      "max_depth=6\n",
      "0.568\t0.571\n",
      "max_depth=7\n",
      "0.565\t0.568\n",
      "max_depth=8\n",
      "0.565\t0.568\n",
      "max_depth=9\n",
      "0.566\t0.570\n",
      "max_depth=10\n",
      "0.557\t0.565\n",
      "max_depth=11\n",
      "0.552\t0.573\n",
      "max_depth=12\n",
      "0.546\t0.567\n",
      "max_depth=13\n",
      "0.547\t0.570\n",
      "max_depth=14\n",
      "0.537\t0.578\n",
      "max_depth=15\n",
      "0.560\t0.588\n",
      "max_depth=16\n",
      "0.530\t0.566\n",
      "max_depth=17\n",
      "0.523\t0.566\n",
      "max_depth=18\n",
      "0.526\t0.570\n",
      "max_depth=19\n",
      "0.524\t0.567\n",
      "max_depth=20\n",
      "0.518\t0.562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.55519796, 0.55536558, 0.55519796, 0.55526501, 0.55560025,\n",
       "        0.56776962, 0.56502062, 0.56502062, 0.56636159, 0.55724295,\n",
       "        0.55218076, 0.5455094 , 0.54735325, 0.53746354, 0.56022663,\n",
       "        0.53025579, 0.52274632, 0.52646753, 0.52418787, 0.51801938]),\n",
       " array([0.55519796, 0.55529475, 0.55519796, 0.55523519, 0.55546233,\n",
       "        0.57117355, 0.56839335, 0.56839335, 0.56989745, 0.56526308,\n",
       "        0.57271943, 0.56734822, 0.57002243, 0.57824708, 0.58798876,\n",
       "        0.56574671, 0.56621291, 0.56956025, 0.56704417, 0.56231454]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = np.zeros((20, ))\n",
    "precisions = np.zeros((20, ))\n",
    "for i, max_depth in enumerate(range(1, 21)):\n",
    "    print(f\"{max_depth=}\")\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracies[i] = accuracy_score(y_test, y_pred)\n",
    "    precisions[i] = precision_score(y_test, y_pred)\n",
    "    print(f\"{accuracies[i]:.3f}\\t{precisions[i]:.3f}\")\n",
    "\n",
    "accuracies, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1, 21)[precisions.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=12, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=12, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=12, random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=12, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticker_SIE.DE</th>\n",
       "      <th>Ticker_TCS.NS</th>\n",
       "      <th>Ticker_TTE</th>\n",
       "      <th>Ticker_V</th>\n",
       "      <th>ticker_type_EU</th>\n",
       "      <th>ticker_type_INDIA</th>\n",
       "      <th>ticker_type_US</th>\n",
       "      <th>split</th>\n",
       "      <th>pred5_clf_10</th>\n",
       "      <th>pred6_best_clf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3648.949951</td>\n",
       "      <td>3584.050049</td>\n",
       "      <td>3594.300049</td>\n",
       "      <td>3594.300049</td>\n",
       "      <td>1571996.0</td>\n",
       "      <td>LT.NS</td>\n",
       "      <td>2024</td>\n",
       "      <td>April</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>3590.050049</td>\n",
       "      <td>3634.149902</td>\n",
       "      <td>3576.050049</td>\n",
       "      <td>3599.500000</td>\n",
       "      <td>3599.500000</td>\n",
       "      <td>3748847.0</td>\n",
       "      <td>LT.NS</td>\n",
       "      <td>2024</td>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>3610.000000</td>\n",
       "      <td>3622.000000</td>\n",
       "      <td>3488.449951</td>\n",
       "      <td>3499.800049</td>\n",
       "      <td>3499.800049</td>\n",
       "      <td>4079696.0</td>\n",
       "      <td>LT.NS</td>\n",
       "      <td>2024</td>\n",
       "      <td>May</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>3522.800049</td>\n",
       "      <td>3527.000000</td>\n",
       "      <td>3441.100098</td>\n",
       "      <td>3463.300049</td>\n",
       "      <td>3463.300049</td>\n",
       "      <td>2614667.0</td>\n",
       "      <td>LT.NS</td>\n",
       "      <td>2024</td>\n",
       "      <td>May</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>3479.399902</td>\n",
       "      <td>3496.000000</td>\n",
       "      <td>3425.000000</td>\n",
       "      <td>3427.750000</td>\n",
       "      <td>3427.750000</td>\n",
       "      <td>3375099.0</td>\n",
       "      <td>LT.NS</td>\n",
       "      <td>2024</td>\n",
       "      <td>May</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open         High          Low        Close  Adj Close_x  \\\n",
       "5422  3639.000000  3648.949951  3584.050049  3594.300049  3594.300049   \n",
       "5423  3590.050049  3634.149902  3576.050049  3599.500000  3599.500000   \n",
       "5424  3610.000000  3622.000000  3488.449951  3499.800049  3499.800049   \n",
       "5425  3522.800049  3527.000000  3441.100098  3463.300049  3463.300049   \n",
       "5426  3479.399902  3496.000000  3425.000000  3427.750000  3427.750000   \n",
       "\n",
       "         Volume Ticker  Year  Month Weekday  ... Ticker_SIE.DE  Ticker_TCS.NS  \\\n",
       "5422  1571996.0  LT.NS  2024  April       1  ...             0              0   \n",
       "5423  3748847.0  LT.NS  2024    May       3  ...             0              0   \n",
       "5424  4079696.0  LT.NS  2024    May       4  ...             0              0   \n",
       "5425  2614667.0  LT.NS  2024    May       0  ...             0              0   \n",
       "5426  3375099.0  LT.NS  2024    May       1  ...             0              0   \n",
       "\n",
       "      Ticker_TTE  Ticker_V  ticker_type_EU  ticker_type_INDIA  ticker_type_US  \\\n",
       "5422           0         0               0                  1               0   \n",
       "5423           0         0               0                  1               0   \n",
       "5424           0         0               0                  1               0   \n",
       "5425           0         0               0                  1               0   \n",
       "5426           0         0               0                  1               0   \n",
       "\n",
       "      split  pred5_clf_10  pred6_best_clf  \n",
       "5422   test           1.0               1  \n",
       "5423   test           1.0               1  \n",
       "5424   test           1.0               1  \n",
       "5425   test           1.0               1  \n",
       "5426   test           1.0               1  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = clf.predict(X_train)\n",
    "test_pred = clf.predict(X_test)\n",
    "\n",
    "pred6_best_clf = np.concatenate([train_pred, test_pred])\n",
    "new_df[\"pred6_best_clf\"] = pred6_best_clf\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer of Question 4 : 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
